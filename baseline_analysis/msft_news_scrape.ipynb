{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape the news sentiments up to a year back for microsoft alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import calendar\n",
    "import os\n",
    "import logging\n",
    "# set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "# create a file handler\n",
    "handler = logging.FileHandler('msft_news_scrape.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "# create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(handler)\n",
    "sns.set()\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "def get_data(parameters):\n",
    "    endpoint = \"https://www.alphavantage.co/query\"\n",
    "    for _ in range(200):\n",
    "        parameters['apikey'] = ''.join(random.choices(string.ascii_uppercase + string.digits, k=15))\n",
    "        # Send a GET request to the API endpoint\n",
    "        response = requests.get(endpoint, params=parameters)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'Note' not in data: \n",
    "                break\n",
    "            logger.warning(f'API key {parameters[\"apikey\"]} has been used too many times. response note: {data[\"Note\"]}')\n",
    "            data = None\n",
    "            time.sleep(1)\n",
    "        else: \n",
    "            logger.error(f'API key {parameters[\"apikey\"]} has returned an error. response note: {response.json()}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_news_in_range(start_time: datetime, day_range: int = 100):\n",
    "    # create a folder to store the sentiments and ignore if it already exists\n",
    "    os.makedirs('sentiments', exist_ok=True)\n",
    "    sentiment_list = []\n",
    "    for i in tqdm(range(day_range)):\n",
    "        # repeat the process for ealiest and latest\n",
    "        for sort_by in ['LATEST', 'EARLIEST', 'RELEVANCE']:\n",
    "            # get the time range\n",
    "            time_to = (start_time - timedelta(days=i)).strftime('%Y%m%dT%H%M')\n",
    "            time_from = (start_time - timedelta(days=i+1)).strftime('%Y%m%dT%H%M')\n",
    "            parameters = {\n",
    "                \"function\": \"NEWS_SENTIMENT\",\n",
    "                'tickers': 'MSFT',\n",
    "                \"time_from\": time_from,\n",
    "                \"time_to\": time_to,\n",
    "                \"sort\": sort_by,\n",
    "                \"limit\": \"200\"\n",
    "            }\n",
    "            # get the news sentiment for the past week\n",
    "            sentiment = get_data(parameters)\n",
    "            if sentiment is None:\n",
    "                logger.error(f'Unnable to fetch sentiment data for {sort_by} from {time_from} to {time_to}')\n",
    "                continue\n",
    "            with open(os.path.join('sentiments', f'sentiments_{sort_by}_{time_from}.json'), 'w') as f:\n",
    "                json.dump(sentiment, f, indent=4)\n",
    "            # append the sentiment to the list\n",
    "            sentiment_list.append(sentiment)\n",
    "    return sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 96/365 [45:30<2:07:30, 28.44s/it]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m day_range \u001b[39m=\u001b[39m \u001b[39m365\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39m# start sweeping from right now, for older sweeps change datetime to datetime.now() - timedelta(days=num_days_past)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m all_news \u001b[39m=\u001b[39m sweep_news_in_range(start_time, day_range)\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36msweep_news_in_range\u001b[1;34m(start_time, day_range)\u001b[0m\n\u001b[0;32m     11\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfunction\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mNEWS_SENTIMENT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtickers\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMSFT\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlimit\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m200\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     19\u001b[0m \u001b[39m# get the news sentiment for the past week\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m sentiment \u001b[39m=\u001b[39m get_data(parameters)\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m sentiment \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnnable to fetch sentiment data for \u001b[39m\u001b[39m{\u001b[39;00msort_by\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mtime_from\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mtime_to\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(parameters)\u001b[0m\n\u001b[0;32m     17\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[39melse\u001b[39;00m: \n\u001b[1;32m---> 19\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAPI key \u001b[39m\u001b[39m{\u001b[39;00mparameters[\u001b[39m\"\u001b[39m\u001b[39mapikey\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m has returned an error. response note: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mjson()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    sent_df = pd.read_csv('sentiments.csv').drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    # get the timedelta between the last date in the csv and the current date\n",
    "    day_range = (datetime.now() - pd.to_datetime(sent_df['time']).iloc[0]).days + 1\n",
    "except FileNotFoundError: \n",
    "    sent_df = pd.DataFrame()\n",
    "    day_range = 365\n",
    "\n",
    "start_time = datetime.now()\n",
    "# # get the datetime of the 10.10.2021\n",
    "# start_time = datetime.strptime('2.3.2022', '%d.%m.%Y')\n",
    "# day_range = 365\n",
    "\n",
    "# start sweeping from right now, for older sweeps change datetime to datetime.now() - timedelta(days=num_days_past)\n",
    "all_news = sweep_news_in_range(start_time, day_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "322a509557a274cb16a0e1e68450414eb651dbdd02be71d467608ec080fac03e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
