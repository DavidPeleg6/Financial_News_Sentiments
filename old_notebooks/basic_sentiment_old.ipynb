{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show basic usage of NER and Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install asent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Sentiment and Entity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "### Uncomment it when the script runs for the first time \n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import flair\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# import asent\n",
    "# import neuralcoref\n",
    "# from textblob import TextBlob\n",
    "# from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# # neural coreference resolution\n",
    "# coef = spacy.load('en_core_web_sm')\n",
    "# neuralcoref.add_to_pipe(coef)\n",
    "# doc = coef(text)\n",
    "# print(doc._.coref_clusters)\n",
    "\n",
    "# get a nlp model to do sentiment analysis on text\n",
    "# model = flair.models.TextClassifier.load('en-sentiment')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "# tokenize input text\n",
    "# sentence = flair.data.Sentence(text)\n",
    "print(sia.polarity_scores(text))\n",
    "# make sentiment prediction\n",
    "# model.predict(sentence)\n",
    "# extract sentiment direction and confidence (label and score) object\n",
    "# sentiment = sentence\n",
    "# print(sentiment)\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp.add_pipe('spacytextblob')\n",
    "# nlp.add_pipe(\"sentencizer\")\n",
    "# nlp.add_pipe('asent_en_v1')\n",
    "# get the sentiment analysis\n",
    "# doc = nlp(text)\n",
    "# asent.visualize(doc, style='analysis')\n",
    "\n",
    "# get a nlp model to do entity recognition on text\n",
    "ner = spacy.load(\"en_core_web_sm\")\n",
    "# entity_analyzer = ner.add_pipe(\"ner\")\n",
    "# ner.add_pipe(entity_analyzer)\n",
    "# get the entity recognition\n",
    "doc = ner(text)\n",
    "# visualize the results with displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate sentiment analysis with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASES = ['Well, this week news broke that they had been in talks with Twitter for a $4 billion acquisition, so it looks like they’re still pretty desirable.',\\\n",
    "           'Wow, how things change.',\\\n",
    "           'Traveloka are poised to become public companies in coming months, kickstarting a coming-out party for Southeast Asia’s long-overlooked internet scene.',\\\n",
    "           'Former DHS Secretary Janet Napolitano spoke with Yahoo Finance about comprehensive immigration reform.']\n",
    "\n",
    "for phrase in PHRASES:\n",
    "  print(f'{phrase}')\n",
    "  print(sia.polarity_scores(phrase))\n",
    "  sentence = flair.data.Sentence(phrase)\n",
    "  model.predict(sentence)\n",
    "  print(sentence.get_label())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News + Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_sentiments(keywrd, startd, sources_list = None, show_all_articles = False):\n",
    "   \n",
    "  news_client = NewsApiClient(api_key= NEWS_API_KEY)\n",
    "  if type(startd) == str:\n",
    "    my_date = datetime.strptime(startd,'%d-%b-%Y')\n",
    "  else:\n",
    "    my_date = startd\n",
    "  # business_en_sources = news_client.get_sources('business','en')\n",
    "  if sources_list:\n",
    "    articles = news_client.get_everything(q = keywrd,\n",
    "                                      from_param = my_date.isoformat(), \n",
    "                                      to = (my_date + timedelta(days = 1)).isoformat(),\n",
    "                                      language=\"en\",\n",
    "                                      sources = \",\".join(sources_list),\n",
    "                                      sort_by=\"relevancy\",\n",
    "                                      page_size = 100)\n",
    "  else:\n",
    "     articles = news_client.get_everything(q = keywrd,\n",
    "                                       from_param = my_date.isoformat(), \n",
    "                                       to = (my_date + timedelta(days = 1)).isoformat(),\n",
    "                                       language=\"en\",\n",
    "                                       sort_by=\"relevancy\",\n",
    "                                       page_size = 100)\n",
    "  article_content = ''\n",
    "\n",
    "  date_sentiments = {}\n",
    "  date_sentiments_list = []\n",
    "  seen = set()\n",
    "  \n",
    "  for article in articles['articles']:\n",
    "    if str(article['title']) in seen:\n",
    "      continue\n",
    "    else:\n",
    "      seen.add(str(article['title']))\n",
    "      article_content = str(article['title']) + '. ' + str(article['description'])      \n",
    "      sentiment = sia.polarity_scores(article_content)['compound']\n",
    "      date_sentiments.setdefault(my_date, []).append(sentiment)\n",
    "      date_sentiments_list.append((sentiment, article['url'],article['title'],article['description']))\n",
    "\n",
    "  date_sentiments_l = sorted(date_sentiments_list, key=lambda tup: tup[0], reverse = True)   \n",
    "  sent_list = list(date_sentiments.values())[0]\n",
    "\n",
    "  return pd.DataFrame(date_sentiments_list, columns=['Sentiment','URL','Title','Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy version when we don't filter the business source -- seems to be relevant though, but the description\n",
    "# Get all sources in en\n",
    "\n",
    "# return_articles = get_articles_sentiments(keywrd= 'Tesla stock' ,startd = '9-Apr-2021',sources_list = None, show_all_articles= True)\n",
    "# return_articles.Sentiment.hist(bins=30,grid=False)\n",
    "# print(return_articles.Sentiment.mean())\n",
    "# print(return_articles.Sentiment.count())\n",
    "# print(return_articles.Description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every Day execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy version when we don't filter the business source -- seems to be relevant though, but the description\n",
    "# Get all sources in en\n",
    "\n",
    "my_date = date.today() - timedelta(days=1) \n",
    "\n",
    "return_articles = get_articles_sentiments(keywrd= 'Tesla stock' ,startd = my_date, sources_list = None, show_all_articles= True)\n",
    "return_articles.Sentiment.hist(bins=30, grid=False)\n",
    "print(return_articles)\n",
    "# print(return_articles.Sentiment.mean())\n",
    "# print(return_articles.Sentiment.count())\n",
    "# print(return_articles.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_articles[\"Date\"] = my_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_articles.sort_values(by='Sentiment', ascending=True)[['Sentiment','URL', 'Description','Title']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_articles.sort_values(by='Sentiment', ascending=False)[['Sentiment','URL', 'Description','Title']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_articles.to_csv(\"TSLA_news_sentiments_\"+my_date.isoformat()+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f65b6c89ae844b8e37150dd3489a420df7650db04d9a50812ad6760db9392a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
