{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create a dataset of the latest news"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import newsapi\n",
    "#from newsapi.newsapi_client import NewsApiClient\n",
    "from datetime import date, timedelta, datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain access key to newAPI\n",
    "* get a free key on the website https://newsapi.org\n",
    "* NEWS_API_KEY = personal api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full output in Colab\n",
    "# https://stackoverflow.com/questions/54692405/output-truncation-in-google-colab\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init news api\n",
    "NEWS_API_KEY = '1900869fa01647fca0bdc19b4550daa0'\n",
    "\n",
    "# '1900869fa01647fca0bdc19b4550daa0'\n",
    "# '71234eb576cd465da04f8050a01be06a'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See all sources available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_client = NewsApiClient(api_key= NEWS_API_KEY)\n",
    "# the list of available categories to choose from\n",
    "print('categories:', newsapi.const.categories)\n",
    "# the list of available sources in the business domain\n",
    "# sources = news_client.get_sources(category='business', language='en', country='us')\n",
    "# source_names = [source['id'] for source in sources['sources']]\n",
    "\n",
    "# dividing the list into large sources that require the entire api call and small sources that can be used together\n",
    "large_sources = ['bloomberg', 'business-insider']\n",
    "small_sources = ['financial-post', 'fortune', 'the-wall-street-journal']\n",
    "source_names = large_sources + small_sources\n",
    "print(source_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all articles from the past month"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can download all articles sorted by date, or all articles sorted by company from the snp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# source_names = large_sources + [\",\".join(small_sources)]\n",
    "\n",
    "# #https://newsapi.org/docs/endpoints/everything\n",
    "# tot_articles = []\n",
    "# # datetime.strptime('10-Apr-2021','%d-%b-%Y')\n",
    "# for i in tqdm(range(31)):\n",
    "#     end_date = date.today() - timedelta(days=i)\n",
    "#     start_date = date.today() - timedelta(days=i+1)\n",
    "#     for source in source_names:\n",
    "#         articles = news_client.get_everything(from_param=start_date.isoformat(), to=end_date.isoformat(), language=\"en\", sources=source, sort_by=\"relevancy\")\n",
    "#         tot_articles.extend(articles['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tot_articles = []\n",
    "# datetime.strptime('10-Apr-2021','%d-%b-%Y')\n",
    "end_date = date.today()\n",
    "start_date = date.today() - timedelta(days=31)\n",
    "articles = news_client.get_everything(from_param=start_date.isoformat(), to=end_date.isoformat(), language=\"en\", sources=\",\".join(source_names), sort_by=\"relevancy\")\n",
    "tot_articles.extend(articles['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tot_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame(tot_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a map function to the source column to get the source name\n",
    "articles_df['source'] = articles_df['source'].map(lambda x: x['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows in a dataframe\n",
    "print(articles_df.shape)\n",
    "# get all unique values in a column\n",
    "print(articles_df['source'].unique())\n",
    "articles_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape websites to get the actual articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tot_articles[0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "# At this point we will have some search results from the API. Take the first search result...\n",
    "first_result = tot_articles[0]\n",
    "\n",
    "# ...and download the HTML for it, again with requests\n",
    "r2 = requests.get(first_result[\"url\"])\n",
    "\n",
    "# Check if the request was successful\n",
    "if r2.status_code == 200:\n",
    "    # We now have the article HTML, so parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "    # Done! The article content can be extracted by searching the HTML\n",
    "    article_content = soup.find(\"div\", {\"id\": \"article-body\"}).text\n",
    "    print(article_content)\n",
    "else:\n",
    "    # Handle unsuccessful request\n",
    "    print(\"Error:\", r2.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results csv for later work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv without index\n",
    "articles_df.to_csv(\"30_days_ago\"+end_date.isoformat()+\".csv\", index=False)\n",
    "\n",
    "articles_df.to_csv(\"30_days_ago\"+end_date.isoformat()+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! zip {\"30_days_ago\"+end_date.isoformat()+\".zip\"} {\"30_days_ago\"+end_date.isoformat()+\".csv\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "322a509557a274cb16a0e1e68450414eb651dbdd02be71d467608ec080fac03e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
